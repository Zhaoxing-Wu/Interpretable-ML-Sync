{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed974de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NNetwork import NNetwork as nn\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import os\n",
    "# Cloud \n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "# =======================================================\n",
    "# Connect to S3 resource\n",
    "# =======================================================\n",
    "# connect to S3 and create resource object\n",
    "s3_resource = boto3.resource(\n",
    "            service_name='s3',\n",
    "            region_name='us-west-1',\n",
    "            aws_access_key_id='AKIAWNJSAXHUWYXA4YJF',\n",
    "            aws_secret_access_key='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'\n",
    "            )\n",
    "s3_client = boto3.client('s3', \n",
    "            aws_access_key_id='AKIAWNJSAXHUWYXA4YJF',\n",
    "            aws_secret_access_key='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')\n",
    "\n",
    "# specify bucket object\n",
    "s3_bucket = s3_resource.Bucket('interpretable-sync')\n",
    "objects = s3_client.list_objects_v2(Bucket='interpretable-sync')\n",
    "allkeys = [obj['Key'] for obj in objects['Contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f7da6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ca_param.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14733ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'motifSampling/SAMPLES-100_NTWK-tree_K-10_PATCHES.pkl' was added to the bucket.\n",
      "'motifSampling/SAMPLES-100_NTWK-tree_K-15_PATCHES.pkl' was added to the bucket.\n",
      "'motifSampling/SAMPLES-100_NTWK-tree_K-20_PATCHES.pkl' was added to the bucket.\n",
      "'motifSampling/SAMPLES-100_NTWK-tree_K-25_PATCHES.pkl' was added to the bucket.\n",
      "'motifSampling/SAMPLES-100_NTWK-tree_K-30_PATCHES.pkl' was added to the bucket.\n"
     ]
    }
   ],
   "source": [
    "#generate tree\n",
    "for num_nodes in [10, 15, 20, 25, 30]:\n",
    "    X = []\n",
    "    for i in range(100):\n",
    "        tree = nx.random_tree(n=num_nodes, seed=i)\n",
    "        X.append(np.array(nx.adjacency_matrix(tree).todense()).reshape(1, -1)[0])\n",
    "    filename = \"motifSampling/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+\"_PATCHES.pkl\"\n",
    "    binary_stream = pickle.dumps(np.array(X).T)\n",
    "    print(\"'\"+filename+\"' was added to the bucket.\")\n",
    "    s3_bucket.put_object(Body=binary_stream, Key=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "840ee5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntwk_names = ['nws-20000-1000-05'] \n",
    "for ntwk in ntwk_names:\n",
    "    for num_nodes in [10]:#, 15, 20, 25, 30]:\n",
    "        #read X\n",
    "        dynamic = \"ghm\"\n",
    "        k=num_nodes\n",
    "        ntwk_filename = \"motifSampling/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+\"_PATCHES.pkl\"\n",
    "        feature_filename = \"motifSampling/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+\"_graph_features.csv\"\n",
    "        dynamics_filename = \"motifDynamics/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(k)+'_DYNAMIC-'+str(dynamic)+'_PARAMS-csv.pkl'\n",
    "        coladj_filename = \"motifDynamics/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(k)+'_COLADJ-'+str(dynamic)+'_PARAMS-csv.pkl'\n",
    "        dynamicstree_filename = \"motifDynamics/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+'_DYNAMIC-'+str(dynamic)+'_PARAMS-csv.pkl'\n",
    "        coladjtree_filename = \"motifDynamics/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+'_COLADJ-'+str(dynamic)+'_PARAMS-csv.pkl'\n",
    "        ntwktree_filename = \"motifSampling/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+\"_PATCHES.pkl\"\n",
    "    \n",
    "        X = pickle.loads(s3_bucket.Object(ntwk_filename).get()['Body'].read())\n",
    "        X_tree = pickle.loads(s3_bucket.Object(ntwktree_filename).get()['Body'].read())\n",
    "        df_feature = pickle.loads(s3_bucket.Object(feature_filename).get()['Body'].read())\n",
    "        df_dynamics = pickle.loads(s3_bucket.Object(dynamics_filename).get()['Body'].read())\n",
    "        df_coladj = pickle.loads(s3_bucket.Object(coladj_filename).get()['Body'].read())\n",
    "        df_dynamicstree = pickle.loads(s3_bucket.Object(dynamicstree_filename).get()['Body'].read())\n",
    "        df_coladjtree = pickle.loads(s3_bucket.Object(coladjtree_filename).get()['Body'].read())\n",
    "        \n",
    "        #X = pd.concat([pd.DataFrame(df_graph.T), df/scale], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e00907",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.path_graph(5, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "575974ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comb = pd.concat([pd.DataFrame(X.T), df_coladj/max(df_coladj.max())], axis=1)\n",
    "X_tree_comb = pd.concat([pd.DataFrame(X_tree.T), df_coladjtree/max(df_coladjtree.max())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f17cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100 #number of samples used for learning dictionary\n",
    "ind_dense = sorted(range(len(df_feature.density)), key=lambda i: df_feature.density[i], reverse=True)[:sample_size]\n",
    "\n",
    "ind_sparse = df_feature[df_feature.is_tree != True].sort_values(by='density').index[:sample_size].tolist()\n",
    "ind_con = df_dynamics[df_dynamics==True].index.tolist()\n",
    "if len(ind_con)>sample_size:\n",
    "    ind_con = ind_con[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f27e765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 4\n",
    "W_dense, H_dense = ALS(X=X_comb.loc[ind_dense,].T.values, \n",
    "                       n_components=r, n_iter=100, a0 = 0, a1 = 0, a12 = 0, H_nonnegativity=True, \n",
    "                       W_nonnegativity=True, compute_recons_error=True, subsample_ratio=1)\n",
    "W_sparse, H_sparse = ALS(X=X_comb.loc[ind_sparse,].T.values, \n",
    "                         n_components=r, n_iter=100, a0 = 0, a1 = 0, a12 = 0, \n",
    "                         H_nonnegativity=True, W_nonnegativity=True, \n",
    "                         compute_recons_error=True, subsample_ratio=1)\n",
    "W_con, H_con = ALS(X=X_comb.loc[ind_con,].T.values, n_components=r, \n",
    "                   n_iter=100, a0 = 0, a1 = 0, a12 = 0, H_nonnegativity=True, \n",
    "                   W_nonnegativity=True, compute_recons_error=True, subsample_ratio=1)\n",
    "W_tree, H_tree = ALS(X=X_tree_comb.T.values, n_components=r, \n",
    "                   n_iter=100, a0 = 0, a1 = 0, a12 = 0, H_nonnegativity=True, \n",
    "                   W_nonnegativity=True, compute_recons_error=True, subsample_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3c8234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.concatenate([W_dense.T, W_sparse.T, W_con.T, W_tree.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "160f3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_dynamics.y\n",
    "base = df_dynamics.baseline_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0acd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pickle.load(open('../data/random_graph/ucla_20walk_graph.pkl', 'rb'))\n",
    "df_dynamics = pd.read_csv(\"../data/dynamics_pairs/fca_k8_ucla_20walk_dynamics.csv\")\n",
    "y = df_dynamics.y\n",
    "base = df_dynamics.baseline_width\n",
    "df_dynamics = df_dynamics.loc[:, 's1_1':'s50_20']\n",
    "df = pd.read_csv(\"../data/dynamics_pairs/fca_k8_ucla_20walk_colored_adj.csv\")\n",
    "df_feature = pd.read_csv(\"../data/random_graph/ucla_20walk_graph_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a577a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 4 # scale down colored adj\n",
    "X = pd.concat([pd.DataFrame(df_graph.T), df/scale], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a42c4460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20400)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b55bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = dict_handcraft(df_feature.diameter, base, X, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "33a56e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234338747099768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression   \n",
    "Y_data = y\n",
    "under_sampler = RandomUnderSampler()\n",
    "X_res, y_res = under_sampler.fit_resample(X_comb.values, Y_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 4, \n",
    "                                                    stratify = y_res)\n",
    "logreg = LogisticRegression()  \n",
    "logreg.fit(np.matmul(W, X_train.T).T, y_train)  \n",
    "#Y_pred = logreg.predict(X[:2, :])  \n",
    "#Y_predict = logreg.predict_proba(X[:2, :])   \n",
    "score = logreg.score(np.matmul(W, X_test.T).T, y_test)  \n",
    "print(score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "32cd2f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.61126015]\n"
     ]
    }
   ],
   "source": [
    "print(logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a1a4e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.31211304 -2.76295886 -1.77618195  0.66129669  0.49026395  2.42391569\n",
      "   1.63936249  1.3407128  -0.72093726  0.64140904  0.79717906  1.176643\n",
      "  -2.46274247 -1.11677433 -1.07849319 -0.00432501]]\n"
     ]
    }
   ],
   "source": [
    "print(logreg.coef_)#/(1+np.exp(logreg.coef_)))\n",
    "#add p-value\n",
    "#random sample\n",
    "#print(logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9451b1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     98\n",
       "False     2\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dynamicstree.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "824d00df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motifDynamics/SAMPLES-10000_NTWK-nws-20000-1000-05_K-10_COLADJ-kura_PARAMS-csv.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loading beta [[ 0.22066799  0.88854784 -0.42416123]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:38<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! pred_type filter\n",
      "['kura', 10, 'nws-20000-1000-05', 0.8982355052335042, 0.8048780487804879, 0.7766599597585513, 0.8558758314855875, 0.8143459915611814, 0.8782383419689119, 0.7516629711751663, 0.8100358422939069, 0.8237250554323725, 0.6009015256588072, 0.7691966267199289, 0.6747128674323535, 0.6291611185086551]\n",
      "motifDynamics/SAMPLES-10000_NTWK-nws-20000-1000-05_K-10_COLADJ-fca_PARAMS-csv.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loading beta [[ 0.80219736  0.45676332 -0.6796472 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:28<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! pred_type filter\n",
      "['fca', 10, 'nws-20000-1000-05', 0.9171785250940724, 0.8677494199535963, 0.9390581717451524, 0.7865429234338747, 0.8560606060606061, 0.9866666666666667, 0.8584686774941995, 0.9181141439205955, 0.9234338747099768, 0.6423529411764706, 0.8871866295264624, 0.7451744979528172, 0.6966109563602599]\n",
      "motifDynamics/SAMPLES-10000_NTWK-nws-20000-1000-05_K-10_COLADJ-ghm_PARAMS-csv.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:00<00:12,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loading beta [[-0.17066545  0.58837836 -0.3638096 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:11<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! pred_type filter\n",
      "['ghm', 10, 'nws-20000-1000-05', 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.75]\n"
     ]
    }
   ],
   "source": [
    "#from helper import *\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NNetwork import NNetwork as nn\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import os\n",
    "# Cloud \n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "# =======================================================\n",
    "# Connect to S3 resource\n",
    "# =======================================================\n",
    "# connect to S3 and create resource object\n",
    "s3_resource = boto3.resource(\n",
    "            service_name='s3',\n",
    "            region_name='us-west-1',\n",
    "            aws_access_key_id='AKIAWNJSAXHUWYXA4YJF',\n",
    "            aws_secret_access_key='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'\n",
    "            )\n",
    "s3_client = boto3.client('s3', \n",
    "            aws_access_key_id='AKIAWNJSAXHUWYXA4YJF',\n",
    "            aws_secret_access_key='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')\n",
    "\n",
    "# specify bucket object\n",
    "s3_bucket = s3_resource.Bucket('interpretable-sync')\n",
    "objects = s3_client.list_objects_v2(Bucket='interpretable-sync')\n",
    "allkeys = [obj['Key'] for obj in objects['Contents']]\n",
    "\n",
    "df_rst = []\n",
    "ntwk_names = ['nws-20000-1000-05']#['Caltech36', 'nws-20000-1000-05', 'UCLA26'] \n",
    "for ntwk in ntwk_names:\n",
    "    for num_nodes in [10]:#, 15, 20, 25, 30]:\n",
    "        #read X\n",
    "        ntwk_filename = \"motifSampling/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+\"_PATCHES.pkl\"\n",
    "        feature_filename = \"motifSampling/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+\"_graph_features.csv\"\n",
    "        xEmbDes = pickle.loads(s3_bucket.Object(ntwk_filename).get()['Body'].read())\n",
    "        df_feature = pickle.loads(s3_bucket.Object(feature_filename).get()['Body'].read())\n",
    "        if ntwk == \"nws-20000-1000-05\":\n",
    "            X = xEmbDes\n",
    "        else:\n",
    "            X = xEmbDes['X']\n",
    "            \n",
    "        for ca in [\"kura\", \"fca\", \"ghm\"]:\n",
    "            temp = [ca, num_nodes, ntwk]\n",
    "            name_dynamics = \"motifDynamics/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+'_DYNAMIC-'+str(ca)+'_PARAMS-csv.pkl'\n",
    "            df_dynamics = pickle.loads(s3_bucket.Object(name_dynamics).get()['Body'].read())\n",
    "            #############################CHANGE########################################\n",
    "            name_coladj = \"motifDynamics/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+'_COLADJ-'+str(ca)+'_PARAMS-csv.pkl'\n",
    "            df_coladj = pickle.loads(s3_bucket.Object(name_coladj).get()['Body'].read())\n",
    "            #############################CHANGE########################################\n",
    "            print(name_coladj+\"\\n\")\n",
    "            \n",
    "            #theory driven\n",
    "            if ntwk == \"nws-20000-1000-05\":\n",
    "                sample_size = 100 #number of samples used for learning dictionary\n",
    "                ind_dense = sorted(range(len(df_feature.density)), key=lambda i: df_feature.density[i], reverse=True)[:sample_size]\n",
    "                ind_sparse = df_feature[df_feature.is_tree != True].sort_values(by='density').index[:sample_size].tolist()\n",
    "                ind_con = df_dynamics[df_dynamics==True].index.tolist()\n",
    "                if len(ind_con)>sample_size:\n",
    "                    ind_con = ind_con[:sample_size]\n",
    "                X_comb = pd.concat([pd.DataFrame(X.T), df_coladj/max(df_coladj.max())], axis=1)\n",
    "\n",
    "\n",
    "                r = 4\n",
    "                W_dense, H_dense = ALS(X=X_comb.loc[ind_dense,].T.values, \n",
    "                                       n_components=r, n_iter=100, a0 = 0, a1 = 0, a12 = 0, H_nonnegativity=True, \n",
    "                                       W_nonnegativity=True, compute_recons_error=True, subsample_ratio=1)\n",
    "                W_sparse, H_sparse = ALS(X=X_comb.loc[ind_sparse,].T.values, \n",
    "                                         n_components=r, n_iter=100, a0 = 0, a1 = 0, a12 = 0, \n",
    "                                         H_nonnegativity=True, W_nonnegativity=True, \n",
    "                                         compute_recons_error=True, subsample_ratio=1)\n",
    "                if ca != \"ghm\": \n",
    "                    dynamicstree_filename = \"motifDynamics/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+'_DYNAMIC-'+str(ca)+'_PARAMS-csv.pkl'\n",
    "                    coladjtree_filename = \"motifDynamics/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+'_COLADJ-'+str(ca)+'_PARAMS-csv.pkl'\n",
    "                    ntwktree_filename = \"motifSampling/SAMPLES-100_NTWK-tree_K-\"+str(num_nodes)+\"_PATCHES.pkl\"\n",
    "                    X_tree = pickle.loads(s3_bucket.Object(ntwktree_filename).get()['Body'].read())\n",
    "                    df_dynamicstree = pickle.loads(s3_bucket.Object(dynamicstree_filename).get()['Body'].read())\n",
    "                    df_coladjtree = pickle.loads(s3_bucket.Object(coladjtree_filename).get()['Body'].read())\n",
    "                    X_tree_comb = pd.concat([pd.DataFrame(X_tree.T), df_coladjtree/max(df_coladjtree.max())], axis=1)\n",
    "\n",
    "                    W_con, H_con = ALS(X=X_comb.loc[ind_con,].T.values, n_components=r, \n",
    "                                       n_iter=100, a0 = 0, a1 = 0, a12 = 0, H_nonnegativity=True, \n",
    "                                       W_nonnegativity=True, compute_recons_error=True, subsample_ratio=1)\n",
    "                    W_tree, H_tree = ALS(X=X_tree_comb.T.values, n_components=r, \n",
    "                                       n_iter=100, a0 = 0, a1 = 0, a12 = 0, H_nonnegativity=True, \n",
    "                                       W_nonnegativity=True, compute_recons_error=True, subsample_ratio=1)\n",
    "                    W = np.concatenate([W_dense.T, W_sparse.T, W_con.T, W_tree.T])\n",
    "                else:\n",
    "                    W = np.concatenate([W_dense.T, W_sparse.T])\n",
    "                s3_bucket.put_object(Body=pickle.dumps(W), \n",
    "                                     Key=\"output/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+'_DYNAMIC-'+str(ca)+'_theory_driven_sdl.pkl')\n",
    "            \n",
    "            y = df_dynamics.y\n",
    "            base = df_dynamics.baseline_width\n",
    "            df_dynamics = df_dynamics[[c for c in df_dynamics.columns if c.startswith('s')]]\n",
    "            df_coladj = pd.concat([pd.DataFrame(X.T), df_coladj/max(df_coladj.max())], axis=1)\n",
    "            \n",
    "            Y_data = y\n",
    "            under_sampler = RandomUnderSampler(random_state=42)\n",
    "            X_res, y_res = under_sampler.fit_resample(df_coladj.values, Y_data)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, \n",
    "                                                                test_size = 0.2, \n",
    "                                                                random_state = 4, \n",
    "                                                                stratify = y_res)\n",
    "            xy_dict = {}\n",
    "            xy_dict[\"X_train\"] = X_train\n",
    "            xy_dict[\"X_test\"] = X_test\n",
    "            xy_dict[\"y_train\"] = y_train\n",
    "            xy_dict[\"y_test\"] = y_test\n",
    "            binary_stream = pickle.dumps(xy_dict)\n",
    "            s3_bucket.put_object(Body=binary_stream, Key=\"sdl_xy/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+'_DYNAMIC-'+str(ca)+'.pkl')\n",
    "            \n",
    "            #data-driven\n",
    "            sdl_filename = \"output/SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+'_DYNAMIC-'+str(ca)+'_sdl.pkl'\n",
    "            xi = 1\n",
    "            iter_avg = 1\n",
    "            beta = 0.5\n",
    "            iteration = 100\n",
    "            r = 2\n",
    "            SDL_BCD_class_new = SDL_BCD(X=[X_train.T, y_train.to_numpy().reshape(-1,1).T],  # data, label\n",
    "                                    X_test=[X_test.T, y_test.to_numpy().reshape(-1,1).T],\n",
    "                                    n_components=r, xi=xi, L1_reg = [0,0,0], L2_reg = [0,0,0], \n",
    "                                    nonnegativity=[True,True,False],full_dim=False)\n",
    "            results_dict_new = SDL_BCD_class_new.fit(iter=iteration, subsample_size=None,\n",
    "                                                            beta = beta,\n",
    "                                                            search_radius_const=np.linalg.norm(X_train),\n",
    "                                                            update_nuance_param=False,\n",
    "                                                            if_compute_recons_error=False, if_validate=False)\n",
    "            temp.append(results_dict_new['AUC'])\n",
    "            temp.append(results_dict_new['Accuracy'])\n",
    "            temp.append(results_dict_new['Precision'])\n",
    "            temp.append(results_dict_new['Recall'])\n",
    "            temp.append(results_dict_new['F_score'])\n",
    "            binary_stream = pickle.dumps(results_dict_new)\n",
    "            s3_bucket.put_object(Body=binary_stream, Key=sdl_filename)\n",
    "\n",
    "            rf = RandomForestClassifier(random_state = 42)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            temp.append(precision_score(y_test, y_pred))\n",
    "            temp.append(recall_score(y_test, y_pred))\n",
    "            temp.append(f1_score(y_test, y_pred))\n",
    "            temp.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            Y_data = y\n",
    "            under_sampler = RandomUnderSampler()\n",
    "            X_res, y_res = under_sampler.fit_resample(pd.concat([df_dynamics, base], axis=1, join='inner').copy(), Y_data)\n",
    "            Y_baseline = X_res.baseline_width\n",
    "            Y_data = y_res\n",
    "            #baseline model\n",
    "            length = len(Y_baseline[Y_baseline==False])\n",
    "            Y_baseline[random.sample(list(Y_baseline[Y_baseline==False].index),length//2)] = True\n",
    "            temp.append(precision_score(Y_data, Y_baseline))\n",
    "            temp.append(recall_score(Y_data, Y_baseline))\n",
    "            temp.append(f1_score(Y_data, Y_baseline))\n",
    "            temp.append(accuracy_score(Y_data, Y_baseline))\n",
    "            print(temp)\n",
    "            df_rst.append(temp)\n",
    "            s3_bucket.put_object(Body=pickle.dumps(temp), Key=\"SAMPLES-10000_NTWK-\"+ntwk+\"_K-\"+str(num_nodes)+'_DYNAMIC-'+str(ca)+'_base_sdl_rf_perm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab40965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
